<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>DynaBridge Splunk Enterprise Export - README | DynaBridge for Splunk</title>
    <style>
        :root {
            --db-header-bg: #14172b;
            --db-nav-dark: #1a1d2e;
            --db-accent-purple: #7c5dc7;
            --db-accent-blue: #4da6e8;
            --db-accent-green: #6abf4b;
            --content-bg: #ffffff;
            --content-card: #f8f9fa;
            --text-dark: #1a1a2e;
            --text-body: #3a3a4a;
            --text-muted: #6c757d;
            --text-light: #ffffff;
            --accent-blue: #1456ff;
            --success: #6abf4b;
            --border-light: #e0e0e0;
        }

        * { box-sizing: border-box; margin: 0; padding: 0; }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Arial, sans-serif;
            line-height: 1.45;
            color: var(--text-dark);
            background: var(--content-bg);
            font-size: 14px;
        }

        /* Header - Compact */
        .header-banner {
            background: var(--db-header-bg);
            padding: 20px 16px;
            text-align: center;
        }
        .header-content { max-width: 900px; margin: 0 auto; }
        .logo-container {
            display: flex;
            align-items: center;
            justify-content: center;
            gap: 12px;
            margin-bottom: 8px;
        }
        .logo-image { height: 36px; width: 36px; object-fit: contain; }
        .logo-text { font-size: 1.5em; font-weight: 700; color: var(--text-light); }
        .logo-text .highlight { color: var(--db-accent-purple); }
        .header-tagline {
            color: rgba(255,255,255,0.6);
            font-size: 0.85em;
            margin-bottom: 10px;
        }
        .version-badge {
            display: inline-flex;
            align-items: center;
            gap: 6px;
            background: rgba(255,255,255,0.08);
            border: 1px solid rgba(255,255,255,0.15);
            padding: 4px 12px;
            border-radius: 12px;
            font-size: 0.75em;
            color: rgba(255,255,255,0.7);
        }
        .version-badge .dot { width: 5px; height: 5px; background: var(--success); border-radius: 50%; }
        .doc-title { margin-top: 12px; padding-top: 10px; border-top: 1px solid rgba(255,255,255,0.1); }
        .doc-title h2 { color: var(--text-light); font-size: 1.1em; font-weight: 600; margin: 0; padding: 0; border: none; }

        /* Navigation - Compact */
        .doc-nav {
            background: var(--db-nav-dark);
            position: sticky;
            top: 0;
            z-index: 100;
            border-bottom: 1px solid rgba(255,255,255,0.05);
        }
        .doc-nav-inner { max-width: 1100px; margin: 0 auto; display: flex; flex-wrap: wrap; justify-content: center; }
        .doc-nav a {
            color: rgba(255,255,255,0.55);
            text-decoration: none;
            font-size: 0.78em;
            font-weight: 500;
            padding: 10px 14px;
            border-bottom: 2px solid transparent;
            transition: all 0.15s;
        }
        .doc-nav a:hover { color: var(--text-light); background: rgba(255,255,255,0.05); border-bottom-color: var(--db-accent-purple); }

        /* Content - Tight */
        .content { max-width: 900px; margin: 0 auto; padding: 20px 24px 30px; }

        /* Typography - Compact */
        h1 {
            color: var(--text-dark);
            font-size: 1.6em;
            font-weight: 700;
            margin-bottom: 4px;
            padding-bottom: 8px;
            border-bottom: 2px solid var(--db-accent-purple);
        }
        h2 {
            color: var(--text-dark);
            font-size: 1.25em;
            font-weight: 600;
            margin-top: 20px;
            margin-bottom: 6px;
            padding-bottom: 4px;
            border-bottom: 1px solid var(--border-light);
        }
        h3 {
            color: var(--text-dark);
            font-size: 1.05em;
            font-weight: 600;
            margin-top: 14px;
            margin-bottom: 4px;
        }
        h4 {
            color: var(--text-dark);
            font-size: 0.95em;
            font-weight: 600;
            margin-top: 10px;
            margin-bottom: 3px;
        }
        h5, h6 {
            color: var(--text-dark);
            font-size: 0.9em;
            font-weight: 600;
            margin-top: 8px;
            margin-bottom: 2px;
        }
        p { margin-bottom: 8px; color: var(--text-body); }
        strong { color: var(--text-dark); font-weight: 600; }
        a { color: var(--accent-blue); text-decoration: none; }
        a:hover { text-decoration: underline; }

        /* Code - Compact */
        pre {
            background: #1e1e2e;
            border: 1px solid #2d2d3d;
            border-radius: 4px;
            padding: 10px 12px;
            overflow-x: auto;
            margin: 8px 0;
            font-family: 'SF Mono', Monaco, Consolas, monospace;
            font-size: 0.82em;
            line-height: 1.4;
            color: #e0e0e0;
        }
        code {
            background: #f0f0f2;
            color: #c7254e;
            padding: 1px 4px;
            border-radius: 3px;
            font-family: 'SF Mono', Monaco, Consolas, monospace;
            font-size: 0.85em;
        }
        pre code { background: none; padding: 0; color: #e0e0e0; }

        /* Tables - Compact */
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 10px 0;
            font-size: 0.88em;
            border: 1px solid var(--border-light);
        }
        th {
            background: var(--db-nav-dark);
            color: var(--text-light);
            padding: 8px 10px;
            text-align: left;
            font-weight: 600;
            font-size: 0.8em;
            text-transform: uppercase;
            letter-spacing: 0.02em;
        }
        td {
            padding: 7px 10px;
            border-bottom: 1px solid var(--border-light);
            color: var(--text-body);
        }
        tr:last-child td { border-bottom: none; }
        tr:nth-child(even) td { background: #f9f9fb; }

        /* Lists - Tight */
        ul, ol { margin: 6px 0; padding-left: 20px; color: var(--text-body); }
        li { margin-bottom: 2px; }
        li::marker { color: var(--db-accent-purple); }

        /* Blockquotes */
        blockquote {
            background: var(--content-card);
            border-left: 3px solid var(--db-accent-purple);
            border-radius: 0 4px 4px 0;
            padding: 8px 12px;
            margin: 10px 0;
        }
        blockquote p { margin-bottom: 0; }

        /* HR - Minimal */
        hr { border: none; border-top: 1px solid var(--border-light); margin: 16px 0; }

        /* Footer - Compact */
        .footer { background: var(--db-nav-dark); padding: 20px 16px; text-align: center; }
        .footer-content { max-width: 600px; margin: 0 auto; }
        .footer-logo { font-size: 1em; font-weight: 700; color: var(--text-light); margin-bottom: 6px; }
        .footer-logo .highlight { color: var(--db-accent-purple); }
        .footer p { color: rgba(255,255,255,0.5); font-size: 0.8em; margin-bottom: 4px; }
        .footer-links { display: flex; justify-content: center; gap: 16px; margin-top: 10px; flex-wrap: wrap; }
        .footer-links a { color: rgba(255,255,255,0.45); font-size: 0.75em; }
        .footer-links a:hover { color: var(--db-accent-purple); text-decoration: none; }
        .footer-copyright {
            margin-top: 12px;
            padding-top: 10px;
            border-top: 1px solid rgba(255,255,255,0.08);
            font-size: 0.7em;
            color: rgba(255,255,255,0.35);
        }

        /* Print */
        @media print {
            .doc-nav, .footer { display: none; }
            .content { max-width: 100%; padding: 12px; }
            .header-banner { padding: 12px; }
        }

        /* Responsive */
        @media (max-width: 768px) {
            .header-banner { padding: 16px 12px; }
            .logo-text { font-size: 1.2em; }
            .logo-image { height: 28px; width: 28px; }
            .content { padding: 16px 12px; }
            h1 { font-size: 1.4em; }
            h2 { font-size: 1.15em; }
            table { font-size: 0.82em; }
            th, td { padding: 6px 8px; }
            .doc-nav a { padding: 8px 10px; font-size: 0.72em; }
        }
    </style>
</head>
<body>
    <header class="header-banner">
        <div class="header-content">
            <div class="logo-container">
                <img class="logo-image" src="dynabridge-symbol.png" alt="DynaBridge" onerror="this.style.display='none'" />
                <div class="logo-text"><span class="highlight">Dyna</span>Bridge for Splunk</div>
            </div>
            <p class="header-tagline">Migration Intelligence Platform for Splunk to Dynatrace</p>
            <span class="version-badge"><span class="dot"></span>Version 4.0.1</span>
            <div class="doc-title"><h2>DynaBridge Splunk Enterprise Export - README</h2></div>
        </div>
    </header>

    <nav class="doc-nav">
        <div class="doc-nav-inner">
            <a href="README-SPLUNK-ENTERPRISE.html">Enterprise README</a>
            <a href="README-SPLUNK-CLOUD.html">Cloud README</a>
            <a href="SPLUNK-ENTERPRISE-EXPORT-SPECIFICATION.html">Enterprise Spec</a>
            <a href="SPLUNK-CLOUD-EXPORT-SPECIFICATION.html">Cloud Spec</a>
            <a href="SCRIPT-GENERATED-ANALYTICS-REFERENCE.html">Analytics Reference</a>
            <a href="EXPORT-SCHEMA.html">Export Schema</a>
        </div>
    </nav>

    <main class="content">
<h1>DynaBridge Splunk Enterprise Export Script</h1>
<h2>READ THIS FIRST - Complete Prerequisites Guide</h2>
<p>
<strong>Version</strong>: 4.0.1
<strong>Last Updated</strong>: January 2026
<strong>Related Documents</strong>: <a href="SCRIPT-GENERATED-ANALYTICS-REFERENCE.md">Script-Generated Analytics Reference</a> | <a href="SPLUNK-ENTERPRISE-EXPORT-SPECIFICATION.md">Enterprise Export Specification</a> | <a href="EXPORT-IMPROVEMENT-ANALYSIS.md">Export Improvement Analysis</a>
</p>
<hr>
<h2>CRITICAL: Where to Run This Script</h2>
<h3>TL;DR - Run It ONCE on the Search Head</h3>
<pre><code>┌─────────────────────────────────────────────────────────────────────────┐
│                    WHERE TO RUN THE EXPORT SCRIPT                        │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                          │
│  ✅ RUN HERE (Primary - Required):                                       │
│     • Search Head (standalone)                                           │
│     • SHC Captain (in Search Head Cluster)                              │
│                                                                          │
│  ⚠️  OPTIONAL (Secondary - Only if needed):                              │
│     • Deployment Server (for forwarder deployment configs)              │
│                                                                          │
│  ❌ DO NOT RUN ON:                                                       │
│     • Indexers / Indexer Cluster peers (SH queries them via REST)       │
│     • Universal Forwarders (configs come from Deployment Server)        │
│     • Heavy Forwarders (unless standalone, not managed by DS)           │
│     • Cluster Manager (SH can get cluster info via REST)                │
│     • License Master (SH can get license info via REST)                 │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘</code></pre>
<h3>Why Only the Search Head?</h3>
<p>
The Search Head is the <strong>authoritative source</strong> for migration-relevant data:
</p>
<table>
<thead>
<tr><th>Data Type</th><th>Lives On</th><th>How SH Gets It</th></tr>
</thead>
<tbody>
<tr><td>Dashboards</td><td>Search Head</td><td>Local files + KV Store</td></tr>
<tr><td>Alerts & Saved Searches</td><td>Search Head</td><td>Local files</td></tr>
<tr><td>Users & Roles</td><td>Search Head</td><td>Local + REST API</td></tr>
<tr><td>Search Macros</td><td>Search Head</td><td>Local files</td></tr>
<tr><td>Index Statistics</td><td>Indexers</td><td><strong>REST API from SH</strong></td></tr>
<tr><td>Cluster Topology</td><td>Cluster Manager</td><td><strong>REST API from SH</strong></td></tr>
<tr><td>License Info</td><td>License Master</td><td><strong>REST API from SH</strong></td></tr>
</tbody>
</table>
<p>
<strong>The Search Head can collect everything via its REST API connections to other components.</strong>
</p>
<h3>When to Run on Deployment Server (Optional)</h3>
<p>
Only run on the Deployment Server if you need:
</p>
<ul>
<li>The <strong>source-of-truth</strong> for forwarder configurations (<code>deployment-apps/</code>)</li>
<li>Server class definitions (<code>serverclass.conf</code>)</li>
<li>Which forwarders receive which app configurations</li>
</ul>
<p>
This is <strong>supplementary</strong> to the main SH export, not a replacement.
</p>
<hr>
<h2>Supported Platforms</h2>
<h3>✅ SUPPORTED: Splunk Enterprise (On-Premises)</h3>
<p>
This script is designed for <strong>Splunk Enterprise</strong> deployments where you have:
</p>
<ul>
<li>SSH/shell access to the Splunk servers</li>
<li>File system access to <code>$SPLUNK_HOME/etc/</code></li>
<li>REST API access (ports 8089 or custom)</li>
</ul>
<p>
Supported architectures:
</p>
<ul>
<li>Standalone (single server)</li>
<li>Distributed (Search Heads + Indexers)</li>
<li>Search Head Cluster (SHC)</li>
<li>Indexer Cluster</li>
<li>With or without Deployment Server</li>
</ul>
<h3>❌ NOT SUPPORTED: Splunk Cloud (Classic or Victoria Experience)</h3>
<p>
<strong>Splunk Cloud does NOT allow SSH access</strong> to the underlying infrastructure.
</p>
<p>
For Splunk Cloud migrations, you need:
</p>
<ol>
<li><strong>REST API-only export</strong> (different script - contact DynaBridge team)</li>
<li><strong>Splunk Cloud admin credentials</strong> with appropriate permissions</li>
<li><strong>Network access</strong> to <code>https://your-stack.splunkcloud.com:8089</code></li>
</ol>
<p>
If you're migrating from Splunk Cloud, please contact the DynaBridge team for:
</p>
<ul>
<li>Splunk Cloud-specific export script</li>
<li>Guidance on Cloud-to-Dynatrace migration patterns</li>
</ul>
<hr>
<h2>What This Document Covers</h2>
<p>
This document explains <strong>everything you need to know</strong> before running the DynaBridge Splunk Export Script, including:
</p>
<ol>
<li><a href="#1-what-this-script-does">What This Script Does</a></li>
<li><a href="#2-server-access-requirements">Server Access Requirements</a></li>
<li><a href="#3-splunk-user-permissions">Splunk User Permissions</a></li>
<li><a href="#4-permissions-by-splunk-deployment-type">Permissions by Splunk Deployment Type</a></li>
<li><a href="#5-pre-flight-checklist">Pre-Flight Checklist</a></li>
<li><a href="#6-what-data-gets-collected">What Data Gets Collected</a></li>
<li><a href="#7-security--privacy-considerations">Security & Privacy Considerations</a></li>
<li><a href="#8-command-line-arguments--automation">Command-Line Arguments & Automation</a> <strong>(NEW in v4.0)</strong></li>
<li><a href="#9-troubleshooting-access-issues">Troubleshooting Access Issues</a></li>
</ol>
<hr>
<h2>1. What This Script Does</h2>
<p>
The DynaBridge Export Script collects configuration data, dashboards, alerts, and usage analytics from your Splunk environment to enable migration to Dynatrace.
</p>
<h3>The Script Collects:</h3>
<table>
<thead>
<tr><th>Category</th><th>What's Collected</th><th>Why It's Needed</th></tr>
</thead>
<tbody>
<tr><td><strong>Configurations</strong></td><td>props.conf, transforms.conf, indexes.conf, inputs.conf</td><td>Understanding data pipeline for OpenPipeline conversion</td></tr>
<tr><td><strong>Dashboards</strong></td><td>Classic XML dashboards + Dashboard Studio JSON</td><td>Visual conversion to Dynatrace apps</td></tr>
<tr><td><strong>Alerts</strong></td><td>savedsearches.conf with all alert definitions</td><td>Migration of monitoring and alerting</td></tr>
<tr><td><strong>Users & RBAC</strong></td><td>Users, roles, groups, object ownership</td><td>Mapping who owns what for migration prioritization</td></tr>
<tr><td><strong>Usage Analytics</strong></td><td>Search frequency, dashboard views, alert triggers</td><td>Identifying high-value assets worth migrating</td></tr>
<tr><td><strong>Index Statistics</strong></td><td>Sizes, retention, sourcetypes, volume metrics</td><td>Capacity planning for Dynatrace buckets</td></tr>
</tbody>
</table>
<h3>The Script Does NOT:</h3>
<ul>
<li>❌ Collect passwords, API tokens, or secrets</li>
<li>❌ Access actual log data or search results</li>
<li>❌ Modify any Splunk configurations</li>
<li>❌ Send data anywhere (all data stays local)</li>
<li>❌ Require network access (runs entirely on the Splunk server)</li>
</ul>
<hr>
<h2>2. Server Access Requirements</h2>
<h3>2.1 How to Access the Server</h3>
<p>
You need <strong>shell access</strong> (SSH or console) to the Splunk server. The script must run directly on the machine where Splunk is installed.
</p>
<pre><code class="language-bash"># Example: SSH to your Splunk server
ssh your_username@splunk-server.company.com

# Or if using a jump host
ssh -J jumphost your_username@splunk-server.company.com</code></pre>
<h3>2.2 Operating System User Requirements</h3>
<table>
<thead>
<tr><th>Requirement</th><th>Details</th></tr>
</thead>
<tbody>
<tr><td><strong>Recommended User</strong></td><td><code>splunk</code> (the user running Splunk)</td></tr>
<tr><td><strong>Alternative</strong></td><td><code>root</code> or any user with read access to <code>$SPLUNK_HOME/etc/</code></td></tr>
<tr><td><strong>Required Permissions</strong></td><td>Read access to Splunk configuration directories</td></tr>
</tbody>
</table>
<h4>Why Run as the <code>splunk</code> User?</h4>
<p>
The <code>splunk</code> user (or whatever user runs your Splunk installation) has guaranteed read access to all configuration files. This is the safest and most reliable option.
</p>
<pre><code class="language-bash"># Switch to splunk user
sudo su - splunk

# Or run script as splunk user
sudo -u splunk bash dynabridge-splunk-export.sh</code></pre>
<h3>2.3 File System Access Required</h3>
<p>
The script needs to READ (not write) from these directories:
</p>
<pre><code>$SPLUNK_HOME/
├── etc/
│   ├── system/local/          # System-level configurations
│   ├── apps/*/                # All app configurations
│   ├── users/*/               # User-level objects (optional)
│   └── deployment-apps/       # (Deployment Server only)
└── var/
    └── log/splunk/audit.log   # (Optional) For usage analytics</code></pre>
<h3>2.4 Checking Your Access</h3>
<p>
Run these commands to verify you have the required access:
</p>
<pre><code class="language-bash"># Check if you can read Splunk configs
ls -la $SPLUNK_HOME/etc/apps/

# Check if you can read a config file
cat $SPLUNK_HOME/etc/system/local/server.conf

# Check if audit.log is readable (optional, for usage analytics)
head -5 $SPLUNK_HOME/var/log/splunk/audit.log</code></pre>
<p>
If any of these fail with "Permission denied", you need to either:
</p>
<ol>
<li>Switch to the <code>splunk</code> user</li>
<li>Run as root</li>
<li>Ask your administrator to add read permissions</li>
</ol>
<hr>
<h2>3. Splunk User Permissions</h2>
<h3>3.1 Why Splunk Credentials Are Needed</h3>
<p>
In addition to OS-level access, the script uses <strong>Splunk's REST API</strong> to collect:
</p>
<ul>
<li>Dashboard Studio dashboards (stored in KV Store, not files)</li>
<li>User and role information</li>
<li>Usage analytics from internal indexes</li>
<li>Cluster and distributed environment information</li>
</ul>
<h3>3.2 Required Splunk Capabilities</h3>
<p>
The Splunk user account used for the export needs these capabilities:
</p>
<table>
<thead>
<tr><th>Capability</th><th>Required?</th><th>What It's Used For</th></tr>
</thead>
<tbody>
<tr><td><code>admin_all_objects</code></td><td><strong>Required</strong></td><td>Access all apps and objects</td></tr>
<tr><td><code>list_users</code></td><td><strong>Required</strong></td><td>Collect user information</td></tr>
<tr><td><code>list_roles</code></td><td><strong>Required</strong></td><td>Collect role definitions</td></tr>
<tr><td><code>rest_access</code></td><td><strong>Required</strong></td><td>Make REST API calls</td></tr>
<tr><td><code>search</code></td><td>Recommended</td><td>Run analytics searches</td></tr>
<tr><td><code>list_indexes</code></td><td>Recommended</td><td>Get index metadata</td></tr>
<tr><td><code>list_inputs</code></td><td>Recommended</td><td>Get data input details</td></tr>
<tr><td><code>list_settings</code></td><td>Recommended</td><td>Get system settings</td></tr>
</tbody>
</table>
<h3>3.3 Checking Your Splunk Permissions</h3>
<p>
Run this search in Splunk to see your capabilities:
</p>
<pre><code class="language-spl">| rest /services/authentication/current-context
| table username, roles, capabilities</code></pre>
<p>
Or use the CLI:
</p>
<pre><code class="language-bash">$SPLUNK_HOME/bin/splunk show user-info</code></pre>
<h3>3.4 Creating a Dedicated Export User (Recommended)</h3>
<p>
For security best practices, create a dedicated user for the export:
</p>
<pre><code class="language-bash"># Create a new role with required capabilities
$SPLUNK_HOME/bin/splunk add role dynabridge_export \
  -capability admin_all_objects \
  -capability list_users \
  -capability list_roles \
  -capability list_indexes \
  -capability list_inputs \
  -capability list_settings \
  -capability rest_access \
  -capability search \
  -auth admin:your_password

# Create the export user with this role
$SPLUNK_HOME/bin/splunk add user dynabridge_user \
  -password &#039;SecurePassword123!&#039; \
  -role dynabridge_export \
  -auth admin:your_password</code></pre>
<h3>3.5 Using the Admin Account</h3>
<p>
If creating a dedicated user isn't possible, you can use any existing admin account. The script will prompt for credentials:
</p>
<pre><code>Enter Splunk admin username [admin]:
Enter Splunk admin password: ********</code></pre>
<hr>
<h2>4. Permissions by Splunk Deployment Type</h2>
<p>
Different Splunk deployment types require different access levels. Identify your environment type and follow the specific requirements below.
</p>
<h3>IMPORTANT: You Only Need to Run on ONE Server</h3>
<pre><code>┌─────────────────────────────────────────────────────────────────────────┐
│                        DISTRIBUTED ENVIRONMENT?                          │
│                                                                          │
│  Q: Do I need to run on SHC, IDXC, DS, each UF, each HWF?               │
│                                                                          │
│  A: NO! Run ONCE on the Search Head (or SHC Captain).                   │
│                                                                          │
│     The Search Head can collect data from all other components via      │
│     REST API. You do NOT need to run on:                                │
│       • Indexers (SH queries them)                                      │
│       • Universal Forwarders (configs come from DS)                     │
│       • Heavy Forwarders (configs come from DS)                         │
│       • Cluster Manager (SH queries it)                                 │
│       • License Master (SH queries it)                                  │
│                                                                          │
│     The ONLY exception: Optionally run on Deployment Server if you      │
│     need forwarder deployment configs (deployment-apps/).               │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘</code></pre>
<h3>4.1 Standalone Splunk Enterprise</h3>
<p>
<strong>Description</strong>: Single server running all Splunk components
</p>
<table>
<thead>
<tr><th>Requirement</th><th>Details</th></tr>
</thead>
<tbody>
<tr><td><strong>Server Access</strong></td><td>SSH to the single Splunk server</td></tr>
<tr><td><strong>OS User</strong></td><td><code>splunk</code> or <code>root</code></td></tr>
<tr><td><strong>Splunk User</strong></td><td>Any admin account</td></tr>
<tr><td><strong>Special Considerations</strong></td><td>None - full access from one server</td></tr>
</tbody>
</table>
<pre><code>┌─────────────────────────────────────┐
│         STANDALONE SERVER           │
│                                     │
│  ┌─────────────────────────────┐   │
│  │  ✅ Run script HERE         │   │
│  │  • Full file system access  │   │
│  │  • Full REST API access     │   │
│  │  • All data available       │   │
│  └─────────────────────────────┘   │
└─────────────────────────────────────┘</code></pre>
<h3>4.2 Distributed Environment (Search Head + Indexers)</h3>
<p>
<strong>Description</strong>: Separate search heads and indexer tiers
</p>
<table>
<thead>
<tr><th>Requirement</th><th>Details</th></tr>
</thead>
<tbody>
<tr><td><strong>Server Access</strong></td><td>SSH to the <strong>Search Head</strong> (NOT indexers)</td></tr>
<tr><td><strong>OS User</strong></td><td><code>splunk</code> or <code>root</code></td></tr>
<tr><td><strong>Splunk User</strong></td><td>Admin account with distributed search access</td></tr>
<tr><td><strong>Special Considerations</strong></td><td>Index stats come from search peers via REST</td></tr>
</tbody>
</table>
<pre><code>┌─────────────────────────────────────────────────────────────┐
│                 DISTRIBUTED ENVIRONMENT                      │
│                                                              │
│  ┌──────────────────┐       ┌─────────────────────────────┐ │
│  │  SEARCH HEAD     │       │  INDEXER CLUSTER            │ │
│  │                  │ REST  │                              │ │
│  │  ✅ Run HERE     ├──────►│  ❌ DO NOT run here         │ │
│  │  • Dashboards    │       │  (SH queries via REST)      │ │
│  │  • Alerts        │       │                              │ │
│  │  • Users/RBAC    │       └─────────────────────────────┘ │
│  │  • Usage stats   │                                       │
│  │  • Index stats   │ ◄── Collected via REST from indexers │
│  └──────────────────┘                                       │
└─────────────────────────────────────────────────────────────┘</code></pre>
<h3>4.3 Search Head Cluster (SHC)</h3>
<p>
<strong>Description</strong>: Multiple search heads in a cluster for high availability
</p>
<table>
<thead>
<tr><th>Requirement</th><th>Details</th></tr>
</thead>
<tbody>
<tr><td><strong>Server Access</strong></td><td>SSH to the <strong>SHC Captain</strong> (preferred) or any member</td></tr>
<tr><td><strong>OS User</strong></td><td><code>splunk</code> or <code>root</code></td></tr>
<tr><td><strong>Splunk User</strong></td><td>Admin account</td></tr>
<tr><td><strong>Special Considerations</strong></td><td>Captain has authoritative view of all shared objects</td></tr>
</tbody>
</table>
<pre><code>┌─────────────────────────────────────────────────────────────┐
│               SEARCH HEAD CLUSTER                            │
│                                                              │
│  ┌──────────────────┐                                       │
│  │  SHC CAPTAIN     │ ◄── Run script HERE (preferred)       │
│  │  • Authoritative  │                                       │
│  │  • All shared KOs │                                       │
│  └────────┬─────────┘                                       │
│           │                                                  │
│  ┌────────┼────────┐                                        │
│  │        │        │                                         │
│  ▼        ▼        ▼                                         │
│ ┌───┐   ┌───┐   ┌───┐                                       │
│ │SH1│   │SH2│   │SH3│  ◄── Or run on any member             │
│ └───┘   └───┘   └───┘      (may miss some shared objects)   │
└─────────────────────────────────────────────────────────────┘</code></pre>
<p>
<strong>Finding the SHC Captain</strong>:
</p>
<pre><code class="language-bash">$SPLUNK_HOME/bin/splunk show shcluster-status -auth admin:password</code></pre>
<h3>4.4 Indexer Cluster</h3>
<p>
<strong>Description</strong>: Clustered indexers with a Cluster Manager
</p>
<table>
<thead>
<tr><th>Requirement</th><th>Details</th></tr>
</thead>
<tbody>
<tr><td><strong>Server Access</strong></td><td>SSH to the <strong>Search Head</strong> (NOT Cluster Manager or indexers)</td></tr>
<tr><td><strong>OS User</strong></td><td><code>splunk</code> or <code>root</code></td></tr>
<tr><td><strong>Splunk User</strong></td><td>Admin account</td></tr>
<tr><td><strong>Special Considerations</strong></td><td>Run on SH; CM only for cluster configs if needed</td></tr>
</tbody>
</table>
<pre><code>┌─────────────────────────────────────────────────────────────┐
│                INDEXER CLUSTER                               │
│                                                              │
│  ┌──────────────────┐                                       │
│  │  SEARCH HEAD     │ ◄── Run script HERE                   │
│  │  • Dashboards    │                                        │
│  │  • Alerts        │                                        │
│  │  • Full export   │                                        │
│  └────────┬─────────┘                                       │
│           │ REST                                             │
│  ┌────────┴────────────────────────────────────────┐        │
│  │           INDEXER CLUSTER                         │        │
│  │  ┌────────────┐                                  │        │
│  │  │ Cluster    │ ◄── Only for cluster topology    │        │
│  │  │ Manager    │     (optional, script handles)   │        │
│  │  └─────┬──────┘                                  │        │
│  │        │                                          │        │
│  │  ┌─────┼─────┬─────┐                             │        │
│  │  ▼     ▼     ▼     ▼                              │        │
│  │ IDX1  IDX2  IDX3  IDX4                           │        │
│  └──────────────────────────────────────────────────┘        │
└─────────────────────────────────────────────────────────────┘</code></pre>
<h3>4.5 Universal Forwarder</h3>
<p>
<strong>Description</strong>: Lightweight agent that only forwards data
</p>
<table>
<thead>
<tr><th>Requirement</th><th>Details</th></tr>
</thead>
<tbody>
<tr><td><strong>Server Access</strong></td><td>SSH to the forwarder host</td></tr>
<tr><td><strong>OS User</strong></td><td><code>splunk</code> or <code>root</code></td></tr>
<tr><td><strong>Splunk User</strong></td><td>Not needed (no REST API on UF)</td></tr>
<tr><td><strong>Special Considerations</strong></td><td><strong>LIMITED EXPORT</strong> - No dashboards, alerts, or users</td></tr>
</tbody>
</table>
<pre><code>┌─────────────────────────────────────────────────────────────┐
│            UNIVERSAL FORWARDER                               │
│                                                              │
│  ┌──────────────────────────────────────────────────────┐   │
│  │  WHAT CAN BE COLLECTED:                               │   │
│  │  ✓ inputs.conf (what logs are being monitored)        │   │
│  │  ✓ outputs.conf (where logs are forwarded to)         │   │
│  │  ✓ props.conf (any local parsing rules)               │   │
│  │  ✓ deploymentclient.conf (deployment server config)   │   │
│  │                                                       │   │
│  │  WHAT CANNOT BE COLLECTED:                            │   │
│  │  ✗ Dashboards (UF has no search capability)           │   │
│  │  ✗ Alerts (UF cannot run searches)                    │   │
│  │  ✗ Users/RBAC (minimal authentication)                │   │
│  │  ✗ Usage analytics (no search history)                │   │
│  └──────────────────────────────────────────────────────┘   │
│                                                              │
│  RECOMMENDATION: Run full export on Search Head instead     │
└─────────────────────────────────────────────────────────────┘</code></pre>
<h3>4.6 Heavy Forwarder</h3>
<p>
<strong>Description</strong>: Full Splunk instance configured for forwarding with parsing
</p>
<table>
<thead>
<tr><th>Requirement</th><th>Details</th></tr>
</thead>
<tbody>
<tr><td><strong>Server Access</strong></td><td>SSH to the Heavy Forwarder</td></tr>
<tr><td><strong>OS User</strong></td><td><code>splunk</code> or <code>root</code></td></tr>
<tr><td><strong>Splunk User</strong></td><td>Admin account (REST API available)</td></tr>
<tr><td><strong>Special Considerations</strong></td><td>Focus on data routing configs; no dashboards</td></tr>
</tbody>
</table>
<h3>4.7 Deployment Server</h3>
<p>
<strong>Description</strong>: Central configuration management for forwarders
</p>
<table>
<thead>
<tr><th>Requirement</th><th>Details</th></tr>
</thead>
<tbody>
<tr><td><strong>Server Access</strong></td><td>SSH to the Deployment Server</td></tr>
<tr><td><strong>OS User</strong></td><td><code>splunk</code> or <code>root</code></td></tr>
<tr><td><strong>Splunk User</strong></td><td>Admin account</td></tr>
<tr><td><strong>Special Considerations</strong></td><td>Collects deployed app configurations</td></tr>
</tbody>
</table>
<pre><code>┌─────────────────────────────────────────────────────────────┐
│              DEPLOYMENT SERVER                               │
│                                                              │
│  ┌──────────────────────────────────────────────────────┐   │
│  │  WHAT CAN BE COLLECTED:                               │   │
│  │  ✓ deployment-apps/* (all apps deployed to forwarders)│   │
│  │  ✓ serverclass.conf (which forwarders get which apps) │   │
│  │  ✓ Server configurations                              │   │
│  │                                                       │   │
│  │  WHAT CANNOT BE COLLECTED:                            │   │
│  │  ✗ Dashboards (typically not on DS)                   │   │
│  │  ✗ Alerts (typically not on DS)                       │   │
│  │  ✗ Usage analytics (no searches run here)             │   │
│  └──────────────────────────────────────────────────────┘   │
│                                                              │
│  NOTE: DS export is useful for understanding forwarder      │
│        configurations and data collection landscape         │
└─────────────────────────────────────────────────────────────┘</code></pre>
<h3>4.8 Splunk Cloud (Classic & Victoria Experience)</h3>
<p>
<strong>⚠️ NOT CURRENTLY SUPPORTED BY THIS SCRIPT</strong>
</p>
<p>
<strong>Description</strong>: Splunk-managed SaaS deployment
</p>
<pre><code>┌─────────────────────────────────────────────────────────────┐
│                  SPLUNK CLOUD                                │
│                                                              │
│  ┌──────────────────────────────────────────────────────┐   │
│  │                                                       │   │
│  │  ❌ THIS SCRIPT DOES NOT SUPPORT SPLUNK CLOUD         │   │
│  │                                                       │   │
│  │  Splunk Cloud (both Classic and Victoria Experience)  │   │
│  │  does NOT allow SSH access to the infrastructure.     │   │
│  │                                                       │   │
│  │  This script requires file system access to:          │   │
│  │    • $SPLUNK_HOME/etc/apps/                          │   │
│  │    • $SPLUNK_HOME/etc/system/local/                  │   │
│  │    • $SPLUNK_HOME/var/log/splunk/                    │   │
│  │                                                       │   │
│  │  Which is not possible in Splunk Cloud.               │   │
│  │                                                       │   │
│  └──────────────────────────────────────────────────────┘   │
│                                                              │
│  FOR SPLUNK CLOUD MIGRATIONS:                               │
│                                                              │
│  Contact the DynaBridge team for:                           │
│  • Splunk Cloud REST API-only export script                 │
│  • Cloud-specific migration guidance                        │
│  • Hybrid (Cloud + On-prem) migration patterns              │
│                                                              │
│  A future version may include --cloud mode for REST-only    │
│  collection from Splunk Cloud environments.                 │
│                                                              │
└─────────────────────────────────────────────────────────────┘</code></pre>
<table>
<thead>
<tr><th>Requirement</th><th>Details</th></tr>
</thead>
<tbody>
<tr><td><strong>Server Access</strong></td><td>❌ Not possible - No SSH to Splunk Cloud</td></tr>
<tr><td><strong>This Script</strong></td><td>❌ Not supported</td></tr>
<tr><td><strong>Alternative</strong></td><td>Contact DynaBridge team for Cloud export tool</td></tr>
</tbody>
</table>
<hr>
<h2>5. Pre-Flight Checklist</h2>
<p>
Before running the export script, verify each item:
</p>
<h3>Server & OS Access</h3>
<ul>
<li>[ ] I have SSH or console access to the appropriate Splunk server</li>
<li>[ ] I can switch to the <code>splunk</code> user OR I have root access</li>
<li>[ ] I can read files in <code>$SPLUNK_HOME/etc/</code></li>
</ul>
<h3>Splunk Credentials</h3>
<ul>
<li>[ ] I have a Splunk admin username and password</li>
<li>[ ] The account has <code>admin<em>all</em>objects</code> capability</li>
<li>[ ] The account has <code>list<em>users</code> and <code>list</em>roles</code> capabilities</li>
</ul>
<h3>Network & Firewall</h3>
<ul>
<li>[ ] The Splunk REST API port (default 8089) is accessible locally</li>
<li>[ ] For Splunk Cloud: I can reach the cloud URL from my machine</li>
</ul>
<h3>Disk Space</h3>
<ul>
<li>[ ] I have at least 500 MB free in <code>/tmp</code> for the export archive</li>
<li>[ ] (Optional) I have space to store the archive before download</li>
</ul>
<h3>Time & Scheduling</h3>
<ul>
<li>[ ] I'm running this during a low-activity period (recommended)</li>
<li>[ ] I have approximately 15-30 minutes for the export to complete</li>
<li>[ ] I've notified relevant teams about the export activity</li>
</ul>
<hr>
<h2>6. What Data Gets Collected</h2>
<h3>6.1 Data Collection Summary</h3>
<table>
<thead>
<tr><th>Data Type</th><th>File/Location</th><th>Sensitivity</th><th>Redacted?</th></tr>
</thead>
<tbody>
<tr><td>Server info</td><td>_systeminfo/server_info.json</td><td>Low</td><td>No</td></tr>
<tr><td>App list</td><td>_systeminfo/installed_apps.json</td><td>Low</td><td>No</td></tr>
<tr><td>props.conf</td><td>[app]/default/props.conf</td><td>Low</td><td>No</td></tr>
<tr><td>transforms.conf</td><td>[app]/default/transforms.conf</td><td>Low</td><td>No</td></tr>
<tr><td>indexes.conf</td><td>_system/local/indexes.conf</td><td>Low</td><td>No</td></tr>
<tr><td>inputs.conf</td><td>[app]/local/inputs.conf</td><td>Medium</td><td>Paths only</td></tr>
<tr><td>savedsearches.conf</td><td>[app]/local/savedsearches.conf</td><td>Medium</td><td>No</td></tr>
<tr><td>Dashboards XML</td><td>[app]/data/ui/views/*.xml</td><td>Low</td><td>No</td></tr>
<tr><td>Dashboard Studio</td><td>dashboard_studio/*.json</td><td>Low</td><td>No</td></tr>
<tr><td>Users list</td><td>_rbac/users.json</td><td>Medium</td><td>Emails included</td></tr>
<tr><td>Roles list</td><td>_rbac/roles.json</td><td>Low</td><td>No</td></tr>
<tr><td>LDAP/SAML config</td><td>_rbac/authentication.conf</td><td>Medium</td><td>Passwords redacted</td></tr>
<tr><td>Usage analytics</td><td>_usage_analytics/*.json</td><td>Low</td><td>Anonymizable</td></tr>
<tr><td>Audit sample</td><td>_audit_sample/audit_sample.log</td><td>Medium</td><td>Optional</td></tr>
</tbody>
</table>
<h3>6.2 Sensitive Data Handling</h3>
<p>
The script automatically redacts or excludes:
</p>
<pre><code>ALWAYS REDACTED:
• Passwords (password = [REDACTED])
• API tokens (token = [REDACTED])
• Private keys (privateKey = [REDACTED])
• Session tokens
• LDAP bind credentials

NEVER COLLECTED:
• Actual log data
• Search results
• KV Store data (except dashboard definitions)
• Encrypted credential storage</code></pre>
<h3>6.3 Optional/Sensitive Collections</h3>
<p>
These require explicit opt-in:
</p>
<table>
<thead>
<tr><th>Collection</th><th>Risk Level</th><th>Contains</th></tr>
</thead>
<tbody>
<tr><td><strong>Lookup Tables</strong></td><td>Medium</td><td>May contain reference data with PII</td></tr>
<tr><td><strong>Audit Log Sample</strong></td><td>Medium</td><td>May contain search queries with sensitive terms</td></tr>
<tr><td><strong>User Email Addresses</strong></td><td>Low</td><td>PII but useful for ownership mapping</td></tr>
</tbody>
</table>
<hr>
<h2>7. Security & Privacy Considerations</h2>
<h3>7.1 Data Stays Local</h3>
<p>
The export script:
</p>
<ul>
<li>Creates a <code>.tar.gz</code> file on the local filesystem</li>
<li>Does NOT transmit data to any external service</li>
<li>Does NOT require internet access (except for Splunk Cloud)</li>
<li>Does NOT modify any Splunk configurations</li>
</ul>
<h3>7.2 Secure the Export File</h3>
<p>
After the export completes:
</p>
<pre><code class="language-bash"># The export file is created with restrictive permissions
ls -la /tmp/splunk_export_*.tar.gz
# -rw------- 1 splunk splunk 150M Jan 15 10:30 splunk_export_...

# Transfer securely (examples)
scp /tmp/splunk_export_*.tar.gz user@secure-host:/path/
rsync -avz --progress /tmp/splunk_export_*.tar.gz user@host:/path/

# Delete after transfer
rm /tmp/splunk_export_*.tar.gz</code></pre>
<h3>7.3 Audit Trail</h3>
<p>
The script logs its activities:
</p>
<pre><code class="language-bash"># View what the script did
cat /tmp/splunk_export_*/export.log

# The audit log shows:
# - What was collected
# - What was skipped
# - Any errors encountered
# - Duration of each step</code></pre>
<h3>7.4 Approval Workflow</h3>
<p>
For regulated environments, consider:
</p>
<ol>
<li><strong>Pre-Approval</strong>: Get written approval before running the export</li>
<li><strong>Witness</strong>: Have a security team member observe the export</li>
<li><strong>Review</strong>: Examine the export contents before sharing</li>
<li><strong>Document</strong>: Log the export as a data handling event</li>
</ol>
<h3>7.5 Data Anonymization (Option 9)</h3>
<p>
When sharing exports with third parties (e.g., migration consultants, Dynatrace Professional Services), enable <strong>Option 9: Anonymize Sensitive Data</strong> to protect privacy while preserving data relationships.
</p>
<p>
<strong>What Gets Anonymized:</strong>
</p>
<table>
<thead>
<tr><th>Data Type</th><th>Original</th><th>Anonymized</th></tr>
</thead>
<tbody>
<tr><td><strong>Email Addresses</strong></td><td><code>john.doe@acme-corp.com</code></td><td><code>user3f8a2c@anon.dynabridge.local</code></td></tr>
<tr><td><strong>Hostnames</strong></td><td><code>splunk-idx01.acme.internal</code></td><td><code>host-7b4c9e12.anon.local</code></td></tr>
<tr><td><strong>IP Addresses</strong></td><td><code>192.168.1.100</code></td><td><code>[IP-REDACTED]</code></td></tr>
<tr><td><strong>IPv6 Addresses</strong></td><td><code>2001:db8::1</code></td><td><code>[IPv6-REDACTED]</code></td></tr>
</tbody>
</table>
<p>
<strong>Key Features:</strong>
</p>
<ul>
<li><strong>Consistent Mapping</strong>: The same original value always produces the same anonymized value, preserving relationships (e.g., all logs from <code>server-01</code> still appear together)</li>
<li><strong>Hash-Based</strong>: Uses SHA-256 hashing, so anonymized values cannot be reversed to reveal originals</li>
<li><strong>Selective</strong>: <code>localhost</code> and <code>127.0.0.1</code> are preserved</li>
<li><strong>Report</strong>: Creates <code><em>anonymization</em>report.json</code> documenting what was anonymized</li>
</ul>
<p>
<strong>When to Use:</strong>
</p>
<pre><code>✅ Use Anonymization When:
  • Sharing export with external consultants
  • Sending to vendor for analysis
  • Including in support tickets
  • Uploading to shared/cloud environments

❌ Skip Anonymization When:
  • Internal use only
  • Migration team needs real hostnames for planning
  • Troubleshooting requires actual IP addresses</code></pre>
<p>
<strong>To Enable:</strong>
</p>
<p>
During Step 4 (Data Categories), enter <code>9</code> to toggle anonymization ON:
</p>
<pre><code>Enter numbers to toggle (e.g., 7,8,9 to add lookups, audit, and anonymization)
Toggle: 9
✓ Data Anonymization: ON - Emails, hostnames, and IPs will be anonymized</code></pre>
<hr>
<h2>8. Command-Line Arguments & Automation</h2>
<p>
<strong>NEW in v4.0</strong>: The script now supports command-line arguments for automation and CI/CD pipelines.
</p>
<h3>8.1 Available Command-Line Arguments</h3>
<table>
<thead>
<tr><th>Argument</th><th>Description</th><th>Example</th></tr>
</thead>
<tbody>
<tr><td><code>-u, --username</code></td><td>Splunk admin username</td><td><code>-u admin</code></td></tr>
<tr><td><code>-p, --password</code></td><td>Splunk admin password</td><td><code>-p MyPassword123</code></td></tr>
<tr><td><code>-h, --host</code></td><td>Splunk host (default: localhost)</td><td><code>-h splunk-server.local</code></td></tr>
<tr><td><code>-P, --port</code></td><td>Splunk REST API port (default: 8089)</td><td><code>-P 8089</code></td></tr>
<tr><td><code>--splunk-home</code></td><td>Splunk installation path</td><td><code>--splunk-home /opt/splunk</code></td></tr>
<tr><td><code>--anonymize</code></td><td>Enable data anonymization (default)</td><td><code>--anonymize</code></td></tr>
<tr><td><code>--no-anonymize</code></td><td>Disable data anonymization</td><td><code>--no-anonymize</code></td></tr>
<tr><td><code>-y, --yes</code></td><td>Auto-confirm all prompts (non-interactive)</td><td><code>-y</code></td></tr>
<tr><td><code>--help</code></td><td>Show help message</td><td><code>--help</code></td></tr>
</tbody>
</table>
<h3>8.2 Non-Interactive Mode (Automation)</h3>
<p>
Use <code>-y</code> or <code>--yes</code> to run the script without any prompts:
</p>
<pre><code class="language-bash"># Fully automated export
./dynabridge-splunk-export.sh \
  -u admin \
  -p &#039;YourPassword&#039; \
  --splunk-home /opt/splunk \
  --anonymize \
  -y</code></pre>
<h3>8.3 Environment Variables</h3>
<p>
The script also supports environment variables (useful for container deployments):
</p>
<table>
<thead>
<tr><th>Variable</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>SPLUNK_USER</code> or <code>SPLUNK_ADMIN_USER</code></td><td>Splunk username</td></tr>
<tr><td><code>SPLUNK_PASSWORD</code> or <code>SPLUNK_ADMIN_PASSWORD</code></td><td>Splunk password</td></tr>
</tbody>
</table>
<p>
Example:
</p>
<pre><code class="language-bash">export SPLUNK_ADMIN_USER=&quot;admin&quot;
export SPLUNK_ADMIN_PASSWORD=&quot;MySecurePassword&quot;
./dynabridge-splunk-export.sh -y --splunk-home /opt/splunk</code></pre>
<h3>8.4 CI/CD Pipeline Integration</h3>
<p>
Example for Jenkins/GitLab CI:
</p>
<pre><code class="language-yaml"># GitLab CI example
splunk_export:
  stage: export
  script:
    - chmod +x dynabridge-splunk-export.sh
    - ./dynabridge-splunk-export.sh \
        -u $SPLUNK_USER \
        -p $SPLUNK_PASSWORD \
        --splunk-home /opt/splunk \
        --anonymize \
        -y
  artifacts:
    paths:
      - dynabridge-export-*.tar.gz</code></pre>
<h3>8.5 Enhanced Anonymization (v4.0)</h3>
<p>
The v4.0 script now anonymizes additional sensitive data types:
</p>
<table>
<thead>
<tr><th>Data Type</th><th>Anonymization Pattern</th></tr>
</thead>
<tbody>
<tr><td>Email addresses</td><td><code>user######@anon.dynabridge.local</code></td></tr>
<tr><td>Hostnames</td><td><code>host-########.anon.local</code></td></tr>
<tr><td>IP addresses</td><td><code>[IP-REDACTED]</code></td></tr>
<tr><td><strong>Webhook URLs</strong></td><td><code>https://webhook.anon.dynabridge.local/hook-###</code></td></tr>
<tr><td><strong>API keys/tokens</strong></td><td><code>[API-KEY-########]</code></td></tr>
<tr><td><strong>PagerDuty keys</strong></td><td><code>[PAGERDUTY-KEY-########]</code></td></tr>
<tr><td><strong>Slack channels</strong></td><td><code>#anon-channel-######</code></td></tr>
<tr><td><strong>Usernames</strong></td><td><code>anon-user-######</code></td></tr>
</tbody>
</table>
<p>
This ensures your export can be safely shared with consultants or support teams.
</p>
<h3>8.6 Enterprise Resilience Features (v4.0.0)</h3>
<p>
<strong>NEW in v4.0.0</strong>: The script now includes comprehensive enterprise-scale features for environments with 4000+ dashboards and 10K+ alerts.
</p>
<h4>Default Settings (Enterprise-Ready)</h4>
<table>
<thead>
<tr><th>Setting</th><th>Default</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>BATCH_SIZE</code></td><td>100</td><td>Items per API request</td></tr>
<tr><td><code>API_TIMEOUT</code></td><td>120s</td><td>Per-request timeout (2 min)</td></tr>
<tr><td><code>MAX_TOTAL_TIME</code></td><td>14400s</td><td>Max runtime (4 hours)</td></tr>
<tr><td><code>MAX_RETRIES</code></td><td>3</td><td>Retry attempts with exponential backoff</td></tr>
<tr><td><code>RATE_LIMIT_DELAY</code></td><td>0.1s</td><td>Delay between API calls (100ms)</td></tr>
<tr><td><code>CHECKPOINT_ENABLED</code></td><td>true</td><td>Enable checkpoint/resume capability</td></tr>
</tbody>
</table>
<h4>Search Head Cluster (SHC) Detection</h4>
<p>
The script automatically detects if running on an SHC Captain and displays a warning:
</p>
<pre><code>┌─────────────────────────────────────────────────────────────────────────┐
│  ⚠️  SEARCH HEAD CLUSTER CAPTAIN DETECTED                               │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                          │
│  You are running this script on the SHC Captain.                        │
│                                                                          │
│  The Captain has additional cluster coordination duties. Running        │
│  intensive operations may temporarily impact cluster performance.       │
│                                                                          │
│  RECOMMENDATIONS:                                                        │
│    • Run during off-peak hours                                          │
│    • Consider running on an SHC Member instead                          │
│    • Monitor cluster health during export                               │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘</code></pre>
<h4>Checkpoint/Resume Capability</h4>
<p>
If the export is interrupted (timeout, network error, Ctrl+C), you can resume:
</p>
<pre><code class="language-bash"># Script detects previous incomplete export
./dynabridge-splunk-export.sh

# Output:
# Found incomplete export from 2025-01-06 14:30:00
# Would you like to resume? (Y/n): Y
# Resuming from: Usage Analytics (step 5 of 8)...</code></pre>
<h4>Export Timing Statistics</h4>
<p>
At completion, the script shows detailed timing:
</p>
<pre><code>┌─────────────────────────────────────────────────────────────────────────┐
│                      EXPORT TIMING STATISTICS                            │
├─────────────────────────────────────────────────────────────────────────┤
│  Total Duration:        5 minutes 4 seconds                              │
│  API Calls:             347                                              │
│  API Retries:           2                                                │
│  API Failures:          0                                                │
│  Batches Completed:     52                                               │
└─────────────────────────────────────────────────────────────────────────┘</code></pre>
<h4>Environment Variable Overrides</h4>
<p>
For very large environments, tune via environment variables:
</p>
<pre><code class="language-bash"># Large environment (5000+ dashboards)
export BATCH_SIZE=50
export API_TIMEOUT=180
./dynabridge-splunk-export.sh

# Or inline
BATCH_SIZE=50 API_TIMEOUT=180 ./dynabridge-splunk-export.sh</code></pre>
<hr>
<h2>9. Troubleshooting Access Issues</h2>
<h3>9.1 "Permission Denied" Reading Files</h3>
<p>
<strong>Symptom</strong>: Script fails with permission errors
</p>
<p>
<strong>Solution</strong>:
</p>
<pre><code class="language-bash"># Check current user
whoami

# Switch to splunk user
sudo su - splunk

# Or run as root
sudo bash dynabridge-splunk-export.sh</code></pre>
<h3>9.2 "Connection Refused" on REST API</h3>
<p>
<strong>Symptom</strong>: REST API calls fail
</p>
<p>
<strong>Check</strong>:
</p>
<pre><code class="language-bash"># Is Splunk running?
$SPLUNK_HOME/bin/splunk status

# Is the management port listening?
netstat -tlnp | grep 8089
ss -tlnp | grep 8089

# Can you connect locally?
curl -k https://localhost:8089/services/server/info -u admin:password</code></pre>
<h3>9.3 "Unauthorized" on REST API</h3>
<p>
<strong>Symptom</strong>: Authentication fails
</p>
<p>
<strong>Check</strong>:
</p>
<pre><code class="language-bash"># Test credentials manually
$SPLUNK_HOME/bin/splunk login -auth admin:password

# Check user capabilities
$SPLUNK_HOME/bin/splunk show user-info -auth admin:password</code></pre>
<h3>9.4 "Capability Not Granted"</h3>
<p>
<strong>Symptom</strong>: REST calls return 403 for certain endpoints
</p>
<p>
<strong>Solution</strong>: Add required capabilities to the user's role:
</p>
<pre><code class="language-bash">$SPLUNK_HOME/bin/splunk edit role your_role \
  -capability list_users \
  -capability list_roles \
  -capability admin_all_objects \
  -auth admin:password</code></pre>
<h3>9.5 Splunk Cloud Access Issues</h3>
<p>
<strong>Symptom</strong>: Cannot reach Splunk Cloud
</p>
<p>
<strong>Check</strong>:
</p>
<pre><code class="language-bash"># Test network connectivity
curl -I https://your-stack.splunkcloud.com

# Check if port is blocked
telnet your-stack.splunkcloud.com 8089

# Verify credentials
curl -k https://your-stack.splunkcloud.com:8089/services/server/info \
  -u your_username:your_password</code></pre>
<h3>9.6 Search Head Cluster Issues</h3>
<p>
<strong>Symptom</strong>: Missing dashboards or knowledge objects
</p>
<p>
<strong>Solution</strong>: Run the script on the SHC Captain:
</p>
<pre><code class="language-bash"># Find the captain
$SPLUNK_HOME/bin/splunk show shcluster-status -auth admin:password | grep -i captain

# SSH to captain and run script there
ssh splunk@shc-captain.company.com
bash dynabridge-splunk-export.sh</code></pre>
<hr>
<h2>Quick Reference Card</h2>
<h3>Where to Run - Decision Tree</h3>
<pre><code>Is it Splunk Cloud?
  └─ YES → ❌ This script not supported. Contact DynaBridge team.
  └─ NO  → Is it a distributed environment?
             └─ YES → Run on Search Head (or SHC Captain)
             └─ NO  → Run on the standalone Splunk server</code></pre>
<h3>Minimum Requirements Summary</h3>
<table>
<thead>
<tr><th>Environment</th><th>Where to Run</th><th>OS User</th><th>Splunk User</th><th>Notes</th></tr>
</thead>
<tbody>
<tr><td>Standalone</td><td>The server</td><td><code>splunk</code></td><td>Admin</td><td>Full export</td></tr>
<tr><td>Distributed</td><td><strong>Search Head only</strong></td><td><code>splunk</code></td><td>Admin</td><td>Queries indexers via REST</td></tr>
<tr><td>SHC</td><td><strong>SHC Captain only</strong></td><td><code>splunk</code></td><td>Admin</td><td>Has all shared objects</td></tr>
<tr><td>Indexer Cluster</td><td><strong>Search Head only</strong></td><td><code>splunk</code></td><td>Admin</td><td>SH queries cluster via REST</td></tr>
<tr><td>With Deployment Server</td><td>SH + optionally DS</td><td><code>splunk</code></td><td>Admin</td><td>DS for forwarder configs</td></tr>
<tr><td>Universal Forwarder</td><td>❌ Don't run here</td><td>-</td><td>-</td><td>Use Deployment Server instead</td></tr>
<tr><td>Heavy Forwarder</td><td>❌ Don't run here</td><td>-</td><td>-</td><td>Use Deployment Server instead</td></tr>
<tr><td>Splunk Cloud</td><td>❌ Not supported</td><td>-</td><td>-</td><td>Different script needed</td></tr>
</tbody>
</table>
<h3>One-Liner Access Test</h3>
<pre><code class="language-bash"># Test everything at once
sudo -u splunk $SPLUNK_HOME/bin/splunk search &quot;| rest /services/authentication/current-context | table username, roles&quot; -auth admin:password</code></pre>
<p>
If this returns your username and roles, you're ready to run the export script!
</p>
<hr>
<h2>Next Steps</h2>
<p>
Once you've verified all requirements:
</p>
<ol>
<li><strong>Download the script</strong>: <code>dynabridge-splunk-export.sh</code></li>
<li><strong>Copy to Splunk server</strong>: <code>scp dynabridge-splunk-export.sh splunk-server:/tmp/</code></li>
<li><strong>Run the script</strong>: <code>sudo -u splunk bash /tmp/dynabridge-splunk-export.sh</code></li>
<li><strong>Follow the prompts</strong>: The script will guide you through each step</li>
<li><strong>Download the export</strong>: Copy the <code>.tar.gz</code> file to your workstation</li>
<li><strong>Upload to DynaBridge</strong>: Open DynaBridge in Dynatrace and upload</li>
</ol>
<hr>
<h2>What to Expect: Step-by-Step Walkthrough</h2>
<p>
This section shows exactly what you'll see when running the script successfully.
</p>
<h3>Step 1: Launch and Welcome Screen</h3>
<p>
When you run <code>./dynabridge-splunk-export.sh</code>, you'll see:
</p>
<pre><code>╔══════════════════════════════════════════════════════════════════════════════╗
║                                                                                ║
║  ██████╗ ██╗   ██╗███╗   ██╗ █████╗ ██████╗ ██████╗ ██╗██████╗  ██████╗ ███████╗ ║
║  ██╔══██╗╚██╗ ██╔╝████╗  ██║██╔══██╗██╔══██╗██╔══██╗██║██╔══██╗██╔════╝ ██╔════╝ ║
║  ██║  ██║ ╚████╔╝ ██╔██╗ ██║███████║██████╔╝██████╔╝██║██║  ██║██║  ███╗█████╗   ║
║  ██║  ██║  ╚██╔╝  ██║╚██╗██║██╔══██║██╔══██╗██╔══██╗██║██║  ██║██║   ██║██╔══╝   ║
║  ██████╔╝   ██║   ██║ ╚████║██║  ██║██████╔╝██║  ██║██║██████╔╝╚██████╔╝███████╗ ║
║  ╚═════╝    ╚═╝   ╚═╝  ╚═══╝╚═╝  ╚═╝╚═════╝ ╚═╝  ╚═╝╚═╝╚═════╝  ╚═════╝ ╚══════╝ ║
║                                                                                ║
║                 🏢  SPLUNK ENTERPRISE EXPORT SCRIPT  🏢                       ║
║                                                                                ║
║          Complete Data Collection for Migration to Dynatrace Gen3            ║
║                        Version 4.0.0                                    ║
║                                                                                ║
╚══════════════════════════════════════════════════════════════════════════════╝

  Documentation: See README-SPLUNK-ENTERPRISE.md for prerequisites

Ready to begin? (Y/n):</code></pre>
<p>
<strong>Action</strong>: Press <code>Y</code> or Enter to continue.
</p>
<h3>Step 2: Pre-Flight Checklist</h3>
<p>
After confirming, you'll see a checklist and system verification:
</p>
<pre><code>╔══════════════════════════════════════════════════════════════════════════════╗
║                     PRE-FLIGHT CHECKLIST                                    ║
║         Please confirm you have the following before continuing            ║
╠══════════════════════════════════════════════════════════════════════════════╣
║                                                                              ║
║  SHELL ACCESS:                                                              ║
║    □  SSH access to Splunk server (or running locally on Splunk server)    ║
║    □  User with read access to $SPLUNK_HOME directory                      ║
║    □  Root/sudo access (may be needed for some configs)                    ║
║                                                                              ║
║  🔒 DATA PRIVACY &amp; SECURITY:                                                ║
║                                                                              ║
║  We do NOT collect or export:                                              ║
║    ✗  User passwords or password hashes                                    ║
║    ✗  API tokens or session keys                                           ║
║    ✗  Private keys or certificates                                         ║
║    ✗  Your actual log data (only metadata/structure)                       ║
║    ✗  SSL certificates or .pem files                                       ║
║                                                                              ║
║  We automatically REDACT:                                                  ║
║    ✓  password = [REDACTED] in all .conf files                             ║
║    ✓  secret = [REDACTED] in outputs.conf                                  ║
║    ✓  pass4SymmKey = [REDACTED] in server.conf                             ║
║    ✓  sslPassword = [REDACTED] in inputs.conf                              ║
╚══════════════════════════════════════════════════════════════════════════════╝

  Quick System Check:
    ✓ bash: 4.4.20(1)-release (4.0+ required)
    ✓ curl: 7.88.1
    ✓ Python: Python 3.9.16 (Splunk bundled)
    ✓ tar: available
    ✓ SPLUNK_HOME: /opt/splunk

Ready to proceed? (Y/n):</code></pre>
<p>
<strong>Action</strong>: Press <code>Y</code> if all checks pass.
</p>
<h3>Step 3: SPLUNK_HOME Detection</h3>
<pre><code>┌─────────────────────────────────────────────────────────────────────────────┐
│ STEP 1: DETECTING SPLUNK INSTALLATION                                       │
└─────────────────────────────────────────────────────────────────────────────┘

◐ Searching for Splunk installation...
✓ Found SPLUNK_HOME: /opt/splunk

  Splunk Version: 9.1.2
  Splunk Build:   abc123def
  Server Name:    splunk-sh01
  Server Role:    search_head

  Is this the correct Splunk installation? (Y/n): Y</code></pre>
<p>
<strong>Action</strong>: Confirm the detected Splunk installation.
</p>
<h3>Step 4: Environment Detection</h3>
<pre><code>┌─────────────────────────────────────────────────────────────────────────────┐
│ STEP 2: DETECTING ENVIRONMENT                                               │
└─────────────────────────────────────────────────────────────────────────────┘

◐ Analyzing Splunk environment...

  Detected Configuration:
  ┌────────────────────────────────────────────────────────────────────────┐
  │  Deployment Type:     Distributed (Search Head)                        │
  │  Search Head Cluster: Yes (Captain)                                    │
  │  Indexer Cluster:     Yes (connected)                                  │
  │  Deployment Server:   No                                               │
  │  License Master:      Connected                                        │
  └────────────────────────────────────────────────────────────────────────┘

  Apps Found: 24 apps in $SPLUNK_HOME/etc/apps/
  Users:      15 users configured

  Is the detected environment correct? (Y/n): Y</code></pre>
<p>
<strong>Action</strong>: Confirm the environment detection is accurate.
</p>
<h3>Step 5: Select Data Categories</h3>
<pre><code>┌─────────────────────────────────────────────────────────────────────────────┐
│ STEP 4: DATA CATEGORIES                                                     │
└─────────────────────────────────────────────────────────────────────────────┘

Select data categories to collect:

  [✓] 1. Configuration Files (props, transforms, indexes, inputs)
      → Required for understanding data pipeline

  [✓] 2. Dashboards (Classic XML + Dashboard Studio JSON)
      → Visual content for conversion to Dynatrace apps

  [✓] 3. Alerts &amp; Saved Searches (savedsearches.conf)
      → Critical for operational continuity

  [✓] 4. Users, Roles &amp; Groups (RBAC data - NO passwords)
      → Usernames and roles only - passwords are NEVER collected

  [✓] 5. Usage Analytics (search frequency, dashboard views)
      → Identifies high-value assets worth migrating

  [✓] 6. Index &amp; Data Statistics
      → Volume metrics for capacity planning

  [ ] 7. Lookup Tables (.csv files)
      → May contain sensitive data - review before including

  [ ] 8. Audit Log Sample (last 10,000 entries)
      → May contain sensitive query content

  [ ] 9. Anonymize Sensitive Data (emails, hostnames, IPs)
      → Replaces real data with consistent fake values
      → RECOMMENDED when sharing export with third parties

  🔒 Privacy: Passwords are NEVER collected. Secrets in .conf files are auto-redacted.

Enter numbers to toggle (e.g., 7,8,9 to add lookups, audit, and anonymization)
Or press Enter to accept defaults [1-6]:</code></pre>
<p>
<strong>Action</strong>: Press Enter to accept defaults, or enter numbers to toggle options.
</p>
<h3>Step 6: Splunk Authentication (for Usage Analytics)</h3>
<pre><code>┌─────────────────────────────────────────────────────────────────────────────┐
│ STEP 5: SPLUNK AUTHENTICATION                                               │
└─────────────────────────────────────────────────────────────────────────────┘

  WHY WE NEED THIS:
  Some data requires accessing Splunk&#039;s REST API, including:
    • Dashboard Studio dashboards (stored in KV Store)
    • User and role information
    • Usage analytics from internal indexes
    • Distributed environment topology

  REQUIRED PERMISSIONS:
  The account needs: admin_all_objects, list_users, list_roles

  SECURITY NOTE:
  Credentials are only used locally and are never stored or transmitted.

Splunk admin username [admin]: admin
Splunk admin password: ••••••••••••

◐ Testing authentication...
✓ Authentication successful

◐ Checking account capabilities...
✓ admin_all_objects: granted
✓ list_users: granted
✓ list_roles: granted
✓ search: granted</code></pre>
<p>
<strong>Action</strong>: Enter Splunk admin credentials.
</p>
<h3>Step 7: Data Collection Progress</h3>
<pre><code>┌─────────────────────────────────────────────────────────────────────────────┐
│ COLLECTING DATA                                                             │
└─────────────────────────────────────────────────────────────────────────────┘

  [1/8] Collecting system information...
✓ Server info collected
✓ License info collected
✓ Installed apps list collected

  [2/8] Collecting configuration files...
✓ apps/search/local/props.conf
✓ apps/search/local/transforms.conf
✓ apps/security_essentials/local/savedsearches.conf
[████████████████████████████████████████] 100% (127 files)

  [3/8] Collecting dashboards...
✓ apps/search/default/data/ui/views/ (12 dashboards)
✓ apps/security_essentials/default/data/ui/views/ (28 dashboards)
✓ Dashboard Studio: 15 dashboards from KV Store

  [4/8] Collecting alerts and saved searches...
✓ Collected 156 saved searches
✓ Identified 47 alerts (alert.track = 1)

  [5/8] Collecting users and roles...
✓ 15 users collected (passwords NOT collected)
✓ 8 roles collected with capabilities

  [6/8] Collecting usage analytics...
◐ Running: Dashboard views (last 30 days)...
✓ Dashboard usage collected
◐ Running: Most active users...
✓ User activity collected
◐ Running: Alert execution history...
✓ Alert statistics collected

  [7/8] Collecting index statistics...
✓ 23 indexes analyzed
✓ Volume metrics collected

  [8/8] Generating manifest and summary...
✓ manifest.json created
✓ dynasplunk-env-summary.md created</code></pre>
<h3>Step 8: Export Complete</h3>
<pre><code>╔══════════════════════════════════════════════════════════════════════════════╗
║                         EXPORT COMPLETE!                                     ║
╠══════════════════════════════════════════════════════════════════════════════╣
║                                                                              ║
║  Export Archive:                                                             ║
║    📦 dynabridge_export_splunk-sh01_20241203_152347.tar.gz                   ║
║                                                                              ║
║  Summary:                                                                    ║
║  ┌──────────────────────────────────────────────────────────────────────┐   ║
║  │  Dashboards:        55 (40 Classic + 15 Studio)                      │   ║
║  │  Alerts:            47                                               │   ║
║  │  Saved Searches:    156                                              │   ║
║  │  Users:             15                                               │   ║
║  │  Roles:             8                                                │   ║
║  │  Apps:              24                                               │   ║
║  │  Indexes:           23                                               │   ║
║  │  Config Files:      127                                              │   ║
║  └──────────────────────────────────────────────────────────────────────┘   ║
║                                                                              ║
║  Duration: 7 minutes 12 seconds                                              ║
║  Archive Size: 8.7 MB                                                        ║
║                                                                              ║
╠══════════════════════════════════════════════════════════════════════════════╣
║                                                                              ║
║  NEXT STEPS:                                                                 ║
║                                                                              ║
║  1. Copy the export to your workstation:                                     ║
║     scp splunk-sh01:/tmp/dynabridge_export_*.tar.gz ./                       ║
║                                                                              ║
║  2. Upload to DynaBridge:                                                    ║
║     Open DynaBridge for Splunk app → Data Sources → Upload Export            ║
║                                                                              ║
║  3. Review the summary report:                                               ║
║     cat dynabridge_export_splunk-sh01_20241203_152347/                       ║
║         dynasplunk-env-summary.md                                            ║
║                                                                              ║
╚══════════════════════════════════════════════════════════════════════════════╝</code></pre>
<h3>What Success Looks Like</h3>
<p>
After a successful export, you'll have a <code>.tar.gz</code> file. Extract it to see:
</p>
<pre><code class="language-bash">$ tar -tzf dynabridge_export_splunk-sh01_20241203_152347.tar.gz | head -25

dynabridge_export_splunk-sh01_20241203_152347/
dynabridge_export_splunk-sh01_20241203_152347/manifest.json
dynabridge_export_splunk-sh01_20241203_152347/dynasplunk-env-summary.md
dynabridge_export_splunk-sh01_20241203_152347/_export.log
dynabridge_export_splunk-sh01_20241203_152347/_systeminfo/
dynabridge_export_splunk-sh01_20241203_152347/_systeminfo/environment.json
dynabridge_export_splunk-sh01_20241203_152347/_systeminfo/server_info.json
dynabridge_export_splunk-sh01_20241203_152347/_systeminfo/license_info.json
dynabridge_export_splunk-sh01_20241203_152347/_rbac/
dynabridge_export_splunk-sh01_20241203_152347/_rbac/users.json
dynabridge_export_splunk-sh01_20241203_152347/_rbac/roles.json
dynabridge_export_splunk-sh01_20241203_152347/_usage_analytics/
dynabridge_export_splunk-sh01_20241203_152347/_usage_analytics/dashboard_views.json
dynabridge_export_splunk-sh01_20241203_152347/_usage_analytics/users_most_active.json
dynabridge_export_splunk-sh01_20241203_152347/_usage_analytics/alert_execution_history.json
dynabridge_export_splunk-sh01_20241203_152347/_indexes/
dynabridge_export_splunk-sh01_20241203_152347/_indexes/index_stats.json
dynabridge_export_splunk-sh01_20241203_152347/search/
dynabridge_export_splunk-sh01_20241203_152347/search/local/props.conf
dynabridge_export_splunk-sh01_20241203_152347/search/local/transforms.conf
dynabridge_export_splunk-sh01_20241203_152347/search/local/savedsearches.conf
dynabridge_export_splunk-sh01_20241203_152347/search/default/data/ui/views/
dynabridge_export_splunk-sh01_20241203_152347/security_essentials/
dynabridge_export_splunk-sh01_20241203_152347/dashboard_studio/</code></pre>
<h3>If Something Goes Wrong</h3>
<p>
If errors occur, you'll see a warning box:
</p>
<pre><code>╔══════════════════════════════════════════════════════════════════════════════╗
║  ⚠️  EXPORT COMPLETED WITH 2 ERRORS                                          ║
╠══════════════════════════════════════════════════════════════════════════════╣
║                                                                              ║
║  Some data could not be collected. See details below:                        ║
║                                                                              ║
║  Errors:                                                                     ║
║    • Permission denied reading /opt/splunk/etc/apps/custom_app/local/        ║
║    • REST API timeout querying indexer cluster status                        ║
║                                                                              ║
║  A troubleshooting report has been generated:                                ║
║    📄 TROUBLESHOOTING.md                                                      ║
║                                                                              ║
║  The export is still usable - only the failed items are missing.             ║
║                                                                              ║
╚══════════════════════════════════════════════════════════════════════════════╝</code></pre>
<p>
Review <code>TROUBLESHOOTING.md</code> in the export directory for specific remediation steps.
</p>
<h3>Verifying the Export</h3>
<p>
After the export completes, verify it's valid:
</p>
<pre><code class="language-bash"># Check the manifest
$ cat dynabridge_export_*/manifest.json | jq &#039;.statistics&#039;
{
  &quot;apps&quot;: 24,
  &quot;dashboards&quot;: 55,
  &quot;alerts&quot;: 47,
  &quot;saved_searches&quot;: 156,
  &quot;users&quot;: 15,
  &quot;roles&quot;: 8,
  &quot;indexes&quot;: 23
}

# Check for errors in the log
$ grep -i error dynabridge_export_*/_export.log
(no output = no errors)

# Verify archive integrity
$ tar -tzf dynabridge_export_*.tar.gz &gt; /dev/null &amp;&amp; echo &quot;Archive OK&quot;
Archive OK</code></pre>
<hr>
<h2>Sample Output Files</h2>
<h3>Example: dynasplunk-env-summary.md</h3>
<p>
This human-readable summary report is generated in the export directory:
</p>
<pre><code class="language-markdown"># DynaSplunk Environment Summary

**Export Date**: 2025-12-03T20:23:47Z
**Hostname**: splunk-sh01.acme-corp.com
**Export Tool Version**: 4.0.0

---

## Environment Overview

| Attribute | Value |
|-----------|-------|
| **Product** | Splunk Enterprise |
| **Role** | Search Head |
| **Architecture** | Distributed |
| **SPLUNK_HOME** | /opt/splunk |

---

## Export Statistics

| Category | Count |
|----------|-------|
| **Applications Exported** | 24 |
| **Dashboards** | 156 |
| **Alerts** | 47 |
| **Users** | 15 |
| **Indexes** | 23 |
| **Errors** | 0 |

---

## Applications Included

- search
- security_essentials
- enterprise_security
- itsi
- splunk_app_for_aws
- splunk_app_for_infrastructure
- Splunk_TA_windows
- Splunk_TA_nix
- phantom
- dashboard_studio
- alert_manager
- monitoring_console
- user-prefs
- learned
- introspection_generator_addon
- custom_security_app
- compliance_reporting
- soc_dashboards
- executive_reports
- it_ops_analytics
- network_monitor
- cloud_security
- devsecops
- ml_toolkit

---

## Data Categories Collected

| Category | Collected |
|----------|-----------|
| Configuration Files | Yes |
| Dashboards | Yes |
| Alerts &amp; Saved Searches | Yes |
| Users &amp; RBAC | Yes |
| Usage Analytics | Yes (30d) |
| Index Statistics | Yes |
| Lookup Tables | Yes |
| Audit Log Sample | Yes |
| Data Anonymization | Yes (emails, hosts, IPs) |

---

## Next Steps

1. Download the export file from this server
2. Open DynaBridge for Splunk in Dynatrace
3. Navigate to: Migration Workspace → Project Initialization
4. Upload the .tar.gz file
5. DynaBridge will analyze your environment and show:
   - Migration readiness assessment
   - Dashboard conversion preview
   - Alert conversion checklist
   - Data pipeline requirements

---

*Generated by DynaBridge Splunk Export Tool v4.0.0*</code></pre>
<h3>Example: manifest.json (Schema)</h3>
<p>
This machine-readable manifest is used by DynaBridge to process your export:
</p>
<pre><code class="language-json">{
  &quot;schema_version&quot;: &quot;3.3&quot;,
  &quot;export_tool&quot;: &quot;dynabridge-splunk-export&quot;,
  &quot;export_tool_version&quot;: &quot;4.0.0&quot;,
  &quot;export_timestamp&quot;: &quot;2025-12-03T20:23:47Z&quot;,
  &quot;export_duration_seconds&quot;: 187,

  &quot;source&quot;: {
    &quot;hostname&quot;: &quot;splunk-sh01&quot;,
    &quot;fqdn&quot;: &quot;splunk-sh01.acme-corp.com&quot;,
    &quot;platform&quot;: &quot;Linux&quot;,
    &quot;platform_version&quot;: &quot;Red Hat Enterprise Linux 8.7&quot;,
    &quot;ip_addresses&quot;: [&quot;10.0.1.50&quot;, &quot;192.168.100.50&quot;]
  },

  &quot;splunk&quot;: {
    &quot;home&quot;: &quot;/opt/splunk&quot;,
    &quot;version&quot;: &quot;9.1.3&quot;,
    &quot;build&quot;: &quot;d95b3299fa65&quot;,
    &quot;flavor&quot;: &quot;enterprise&quot;,
    &quot;role&quot;: &quot;search_head&quot;,
    &quot;architecture&quot;: &quot;distributed&quot;,
    &quot;license_type&quot;: &quot;enterprise&quot;,
    &quot;cluster_label&quot;: &quot;acme-production&quot;,
    &quot;is_cloud&quot;: false
  },

  &quot;collection&quot;: {
    &quot;configs&quot;: true,
    &quot;dashboards&quot;: true,
    &quot;alerts&quot;: true,
    &quot;rbac&quot;: true,
    &quot;usage_analytics&quot;: true,
    &quot;usage_period&quot;: &quot;30d&quot;,
    &quot;indexes&quot;: true,
    &quot;lookups&quot;: true,
    &quot;audit_sample&quot;: true
  },

  &quot;statistics&quot;: {
    &quot;apps_exported&quot;: 24,
    &quot;dashboards_classic&quot;: 128,
    &quot;dashboards_studio&quot;: 28,
    &quot;dashboards_total&quot;: 156,
    &quot;alerts&quot;: 47,
    &quot;saved_searches&quot;: 234,
    &quot;users&quot;: 15,
    &quot;roles&quot;: 8,
    &quot;indexes&quot;: 23,
    &quot;lookups&quot;: 34,
    &quot;config_files&quot;: 156,
    &quot;errors&quot;: 0,
    &quot;warnings&quot;: 2,
    &quot;total_files&quot;: 512,
    &quot;total_size_bytes&quot;: 8847234
  },

  &quot;apps&quot;: [
    {
      &quot;name&quot;: &quot;security_essentials&quot;,
      &quot;dashboards&quot;: 32,
      &quot;alerts&quot;: 18,
      &quot;saved_searches&quot;: 45,
      &quot;lookups&quot;: 8
    },
    {
      &quot;name&quot;: &quot;enterprise_security&quot;,
      &quot;dashboards&quot;: 24,
      &quot;alerts&quot;: 12,
      &quot;saved_searches&quot;: 67,
      &quot;lookups&quot;: 12
    },
    {
      &quot;name&quot;: &quot;itsi&quot;,
      &quot;dashboards&quot;: 18,
      &quot;alerts&quot;: 8,
      &quot;saved_searches&quot;: 34,
      &quot;lookups&quot;: 4
    }
  ],

  &quot;usage_intelligence&quot;: {
    &quot;summary&quot;: {
      &quot;dashboards_never_viewed&quot;: 23,
      &quot;alerts_never_fired&quot;: 11,
      &quot;users_inactive_30d&quot;: 3,
      &quot;alerts_with_failures&quot;: 4
    },
    &quot;volume&quot;: {
      &quot;avg_daily_gb&quot;: 127.4,
      &quot;peak_daily_gb&quot;: 245.8,
      &quot;total_30d_gb&quot;: 3822.5,
      &quot;top_indexes_by_volume&quot;: [
        {&quot;index&quot;: &quot;main&quot;, &quot;total_gb&quot;: 1245.6},
        {&quot;index&quot;: &quot;security&quot;, &quot;total_gb&quot;: 876.3},
        {&quot;index&quot;: &quot;windows&quot;, &quot;total_gb&quot;: 543.2},
        {&quot;index&quot;: &quot;linux&quot;, &quot;total_gb&quot;: 412.8},
        {&quot;index&quot;: &quot;network&quot;, &quot;total_gb&quot;: 298.4}
      ],
      &quot;top_sourcetypes_by_volume&quot;: [
        {&quot;sourcetype&quot;: &quot;WinEventLog:Security&quot;, &quot;total_gb&quot;: 567.3},
        {&quot;sourcetype&quot;: &quot;syslog&quot;, &quot;total_gb&quot;: 423.8},
        {&quot;sourcetype&quot;: &quot;access_combined&quot;, &quot;total_gb&quot;: 312.4},
        {&quot;sourcetype&quot;: &quot;aws:cloudtrail&quot;, &quot;total_gb&quot;: 245.6}
      ],
      &quot;top_hosts_by_volume&quot;: [
        {&quot;host&quot;: &quot;dc01.acme-corp.com&quot;, &quot;total_gb&quot;: 234.5},
        {&quot;host&quot;: &quot;web-prod-01&quot;, &quot;total_gb&quot;: 187.3},
        {&quot;host&quot;: &quot;app-server-cluster&quot;, &quot;total_gb&quot;: 156.8}
      ]
    },
    &quot;prioritization&quot;: {
      &quot;top_dashboards&quot;: [
        {&quot;dashboard&quot;: &quot;security_overview&quot;, &quot;app&quot;: &quot;security_essentials&quot;, &quot;views&quot;: 3847},
        {&quot;dashboard&quot;: &quot;executive_summary&quot;, &quot;app&quot;: &quot;executive_reports&quot;, &quot;views&quot;: 2156},
        {&quot;dashboard&quot;: &quot;soc_main&quot;, &quot;app&quot;: &quot;soc_dashboards&quot;, &quot;views&quot;: 1923},
        {&quot;dashboard&quot;: &quot;incident_tracker&quot;, &quot;app&quot;: &quot;enterprise_security&quot;, &quot;views&quot;: 1654}
      ],
      &quot;top_users&quot;: [
        {&quot;user&quot;: &quot;admin&quot;, &quot;searches&quot;: 8934},
        {&quot;user&quot;: &quot;soc_analyst1&quot;, &quot;searches&quot;: 5621},
        {&quot;user&quot;: &quot;security_lead&quot;, &quot;searches&quot;: 4532},
        {&quot;user&quot;: &quot;it_ops&quot;, &quot;searches&quot;: 3245}
      ],
      &quot;top_alerts&quot;: [
        {&quot;alert&quot;: &quot;Failed Authentication&quot;, &quot;app&quot;: &quot;security_essentials&quot;, &quot;fires&quot;: 1234},
        {&quot;alert&quot;: &quot;High CPU Usage&quot;, &quot;app&quot;: &quot;itsi&quot;, &quot;fires&quot;: 567},
        {&quot;alert&quot;: &quot;Suspicious Network Activity&quot;, &quot;app&quot;: &quot;enterprise_security&quot;, &quot;fires&quot;: 345}
      ],
      &quot;top_sourcetypes&quot;: [
        {&quot;sourcetype&quot;: &quot;WinEventLog:Security&quot;, &quot;searches&quot;: 12453},
        {&quot;sourcetype&quot;: &quot;syslog&quot;, &quot;searches&quot;: 8976},
        {&quot;sourcetype&quot;: &quot;access_combined&quot;, &quot;searches&quot;: 5643}
      ],
      &quot;top_indexes&quot;: [
        {&quot;index&quot;: &quot;main&quot;, &quot;searches&quot;: 15678},
        {&quot;index&quot;: &quot;security&quot;, &quot;searches&quot;: 12345},
        {&quot;index&quot;: &quot;_audit&quot;, &quot;searches&quot;: 4532}
      ]
    },
    &quot;elimination_candidates&quot;: {
      &quot;dashboards_never_viewed_count&quot;: 23,
      &quot;alerts_never_fired_count&quot;: 11,
      &quot;users_inactive_count&quot;: 3,
      &quot;note&quot;: &quot;See _usage_analytics/ for full lists of candidates&quot;
    },
    &quot;ownership_mapping&quot;: {
      &quot;apps_by_owner&quot;: [
        {&quot;owner&quot;: &quot;security_team&quot;, &quot;apps&quot;: [&quot;security_essentials&quot;, &quot;enterprise_security&quot;]},
        {&quot;owner&quot;: &quot;it_ops&quot;, &quot;apps&quot;: [&quot;itsi&quot;, &quot;splunk_app_for_infrastructure&quot;]},
        {&quot;owner&quot;: &quot;admin&quot;, &quot;apps&quot;: [&quot;search&quot;, &quot;monitoring_console&quot;]}
      ],
      &quot;dashboards_by_owner&quot;: [
        {&quot;owner&quot;: &quot;soc_analyst1&quot;, &quot;count&quot;: 12},
        {&quot;owner&quot;: &quot;security_lead&quot;, &quot;count&quot;: 8},
        {&quot;owner&quot;: &quot;admin&quot;, &quot;count&quot;: 45}
      ]
    }
  },

  &quot;cluster&quot;: {
    &quot;is_clustered&quot;: true,
    &quot;cluster_mode&quot;: &quot;search_head_cluster&quot;,
    &quot;cluster_label&quot;: &quot;acme-production&quot;,
    &quot;captain&quot;: &quot;splunk-sh01.acme-corp.com&quot;,
    &quot;members&quot;: [
      &quot;splunk-sh01.acme-corp.com&quot;,
      &quot;splunk-sh02.acme-corp.com&quot;,
      &quot;splunk-sh03.acme-corp.com&quot;
    ]
  }
}</code></pre>
<p>
This manifest enables DynaBridge to:
</p>
<ul>
<li><strong>Prioritize migration</strong> based on actual usage data (most-viewed dashboards first)</li>
<li><strong>Identify elimination candidates</strong> (unused dashboards/alerts - don't migrate waste)</li>
<li><strong>Estimate data volume</strong> for Dynatrace ingestion planning and licensing</li>
<li><strong>Map ownership</strong> to coordinate with stakeholders</li>
<li><strong>Understand cluster topology</strong> for multi-node environments</li>
</ul>
<hr>
<p>
<em>For support, contact your DynaBridge administrator or visit the documentation portal.</em>
</p>
    </main>

    <footer class="footer">
        <div class="footer-content">
            <div class="footer-logo"><span class="highlight">Dyna</span>Bridge for Splunk</div>
            <p>Migration intelligence for Splunk to Dynatrace Gen3 Grail</p>
            <div class="footer-links">
                <a href="README-SPLUNK-ENTERPRISE.html">Enterprise Guide</a>
                <a href="README-SPLUNK-CLOUD.html">Cloud Guide</a>
                <a href="SCRIPT-GENERATED-ANALYTICS-REFERENCE.html">Analytics</a>
                <a href="EXPORT-SCHEMA.html">Schema</a>
            </div>
            <p class="footer-copyright">DynaBridge Project &bull; Built for Dynatrace Gen3 Platform</p>
        </div>
    </footer>
</body>
</html>
